{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from sbi import utils as utils\n",
    "from sbi.analysis import pairplot\n",
    "from sbi.inference import NSPE, simulate_for_sbi\n",
    "from sbi.utils.user_input_checks import (\n",
    "    check_sbi_inputs,\n",
    "    process_prior,\n",
    "    process_simulator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dim = 2\n",
    "prior = utils.BoxUniform(low=-2 * torch.ones(num_dim), high=2 * torch.ones(num_dim))\n",
    "\n",
    "def simulator(theta):\n",
    "    # linear gaussian\n",
    "    return theta + 1.0 + torch.randn_like(theta) * 0.1\n",
    "\n",
    "# Check prior, simulator, consistency\n",
    "prior, num_parameters, prior_returns_numpy = process_prior(prior)\n",
    "simulator = process_simulator(simulator, prior, prior_returns_numpy)\n",
    "check_sbi_inputs(simulator, prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference object. Here, NPE is used.\n",
    "inference = NSPE(prior=None, sde_type=\"ve\")\n",
    "\n",
    "# generate simulations and pass to the inference object\n",
    "theta, x = simulate_for_sbi(simulator, proposal=prior, num_simulations=5_000)\n",
    "inference = inference.append_simulations(theta, x)\n",
    "\n",
    "# train the density estimator and build the posterior\n",
    "score_estimator = inference.train(stop_after_epochs=1000)\n",
    "posterior = inference.build_posterior(score_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import plot_summary\n",
    "\n",
    "plot_summary(inference, tags=[\"training_loss\", \"validation_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior = inference.build_posterior(score_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_true = prior.sample((1,))\n",
    "# generate our observation\n",
    "x_obs = simulator(theta_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.samplers.score.predictors import DDIM, EulerMaruyama \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posterior.sample((10000,), x=x_obs, steps=500)*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.rand(10000, 2) * 4 - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probs = posterior.log_prob(samples, x=x_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.set_default_x(x_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = posterior.map(learning_rate=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = posterior.sample((10000,), x=x_obs)\n",
    "#samples = x.detach()\n",
    "_ = pairplot(samples, points=theta_true, limits=[[-2, 2], [-2, 2], [-2, 2]], figsize=(6, 6), labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zuko.transforms import FreeFormJacobianTransform\n",
    "\n",
    "def build_freeform_jacobian_transform(x_o, atol=1e-5, rtol=1e-6, exact=True):\n",
    "    # Create a freeform jacobian transformation\n",
    "    phi = score_estimator.parameters()\n",
    "    def f(t,x):\n",
    "        score = score_estimator(input=x, condition=x_o, time=t)\n",
    "        f = score_estimator.drift_fn(x,t)\n",
    "        g = score_estimator.diffusion_fn(x,t)\n",
    "        v = f - 0.5*g**2 * score\n",
    "        return v\n",
    "\n",
    "    transform = FreeFormJacobianTransform(\n",
    "        f=f, t0= score_estimator.T_min, t1=score_estimator.T_max, phi=phi, atol=atol, rtol=rtol, exact=exact\n",
    "    )\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_o=x_obs\n",
    "\n",
    "def f(t,x):\n",
    "    t = torch.atleast_1d(t)\n",
    "    score = score_estimator(input=x, condition=x_o, time=t)\n",
    "    f = score_estimator.drift_fn(x, t)\n",
    "    g = score_estimator.diffusion_fn(x,t)\n",
    "\n",
    "\n",
    "    v = f -  0.5*g**2*score\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = build_freeform_jacobian_transform(x_o=x_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = t.inv(torch.randn(1000,3)*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.inv.call_and_ladj(torch.randn(1000,3)*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.linspace(1,1e-5,1000)\n",
    "x0s = torch.randn(100,3)*5.\n",
    "x = x0s\n",
    "for i in range(1000):\n",
    "    x -= f(ts[i],x) * 1e-3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = x.detach()\n",
    "\n",
    "\n",
    "_ = pairplot(samples, points=theta_true, limits=[[-2, 2], [-2, 2], [-2, 2]], figsize=(6, 6), labels=[r\"$\\theta_1$\", r\"$\\theta_2$\", r\"$\\theta_3$\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sbibm --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbibm.tasks import get_task\n",
    "\n",
    "task = get_task(\"slcp\")\n",
    "\n",
    "prior = task.get_prior_dist()\n",
    "simulator = task.get_simulator()\n",
    "\n",
    "thetas = prior.sample((100_000,))\n",
    "xs = simulator(thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference object. Here, NPE is used.\n",
    "inference = NSPE(prior=prior, sde_type=\"subvp\")\n",
    "inference = inference.append_simulations(thetas, xs)\n",
    "\n",
    "# train the density estimator and build the posterior\n",
    "score_estimator = inference.train(stop_after_epochs=50, training_batch_size=100)\n",
    "posterior = inference.build_posterior(score_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.analysis import plot_summary\n",
    "\n",
    "_ = plot_summary(inference, tags=[\"training_loss\", \"validation_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_estimator.std_fn(score_estimator.T_min*torch.ones((1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi.utils.metrics import c2st \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in range(1,2):\n",
    "    ref_samples = task.get_reference_posterior_samples(i)\n",
    "    x_obs = task.get_observation(i)\n",
    "    samples = posterior.sample((10000,), x=x_obs, steps=1000)\n",
    "    print(c2st(ref_samples, samples))\n",
    "    plt.scatter(samples[:,0], samples[:,1],s=1)\n",
    "    plt.scatter(ref_samples[:,0], ref_samples[:,1], s=1)\n",
    "    plt.xlim(-1,1)\n",
    "    plt.ylim(-1,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is part of sbi, a toolkit for simulation-based inference. sbi is licensed\n",
    "# under the Apache License Version 2.0, see <https://www.apache.org/licenses/>\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import pytest\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from sbi.neural_nets.score_nets import build_score_estimator\n",
    "from sbi.inference.potentials.score_based_potential import (\n",
    "    score_estimator_based_potential_gradient,\n",
    ")\n",
    "from sbi.samplers.score.score import Diffuser\n",
    "\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    \"sde_type\",\n",
    "    [\n",
    "        \"vp\",\n",
    "        \"ve\",\n",
    "        \"subvp\",\n",
    "    ],\n",
    ")\n",
    "@pytest.mark.parametrize(\"input_event_shape\", ((1,), (4,)))\n",
    "@pytest.mark.parametrize(\"std\", (1.0, 0.1))\n",
    "def test_score_estimator_forward_shapes(sde_type, input_event_shape, std):\n",
    "\n",
    "    mean0 = torch.zeros(input_event_shape)\n",
    "    std0 = std * torch.ones(input_event_shape)\n",
    "\n",
    "    score_fn = _build_gaussian_score_estimator(sde_type, input_event_shape, mean0, std0)\n",
    "\n",
    "    sampler = Diffuser(score_fn, \"euler_maruyama\", None)\n",
    "\n",
    "    T_min = score_fn.score_estimator.T_min\n",
    "    T_max = score_fn.score_estimator.T_max\n",
    "    ts = torch.linspace(T_max, T_min, 1000)\n",
    "    samples = sampler.run(10_000, ts)\n",
    "\n",
    "    mean_est = samples[0].mean(0)\n",
    "    std_est = samples[0].std(0)\n",
    "\n",
    "    # TODO: Fix this\n",
    "\n",
    "    # print(mean_est, std_est)\n",
    "    # assert torch.allclose(mean_est, torch.zeros_like(mean_est), rtol=1e-3)\n",
    "    # assert torch.allclose(std_est, torch.ones_like(mean_est) * std, rtol=1e-3)\n",
    "\n",
    "\n",
    "\n",
    "def _build_gaussian_score_estimator(\n",
    "    sde_type: str,\n",
    "    input_event_shape: Tuple[int],\n",
    "    mean0: Tensor,\n",
    "    std0: Tensor,\n",
    "):\n",
    "    \"\"\"Helper function for all tests that deal with shapes of density estimators.\"\"\"\n",
    "\n",
    "    # Use discrete thetas such that categorical density esitmators can also use them.\n",
    "    building_thetas = (\n",
    "        torch.randn((1000, *input_event_shape), dtype=torch.float32) * std0\n",
    "        + mean0\n",
    "    )\n",
    "    building_xs = torch.ones((1000, 1))\n",
    "\n",
    "    class DummyNet(torch.nn.Module):\n",
    "        def forward(self, x):\n",
    "            return torch.zeros((x.shape[0], *input_event_shape))\n",
    "    \n",
    "\n",
    "    score_estimator = build_score_estimator(\n",
    "        building_thetas,\n",
    "        building_xs,\n",
    "        sde_type=sde_type,\n",
    "        score_net=DummyNet(),\n",
    "    )\n",
    "    \n",
    "\n",
    "    score_fn, _ = score_estimator_based_potential_gradient(\n",
    "        score_estimator, prior=None, x_o=torch.ones((1,))\n",
    "    )\n",
    "\n",
    "    return score_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_event_shape = (1,)\n",
    "std = 1.\n",
    "sde_type=\"ve\"\n",
    "\n",
    "mean0 = torch.zeros(input_event_shape)\n",
    "std0 = std * torch.ones(input_event_shape)\n",
    "\n",
    "score_fn = _build_gaussian_score_estimator(sde_type, input_event_shape, mean0, std0)\n",
    "\n",
    "sampler = Diffuser(score_fn, \"ddim\", None)\n",
    "\n",
    "T_min = score_fn.score_estimator.T_min\n",
    "T_max = score_fn.score_estimator.T_max\n",
    "ts = torch.linspace(T_max, T_min, 500)\n",
    "samples = sampler.run(10_000, ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_est = samples[0].mean(0)\n",
    "std_est = samples[0].std(0)\n",
    "\n",
    "print(mean_est, std_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_fn(torch.ones((1,)),1e-3* torch.ones((1,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(samples.flatten().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sbi import utils\n",
    "from sbi.inference import SNPE, infer\n",
    "\n",
    "import torch\n",
    "\n",
    "# Example is taken from 00_getting_started.ipynb\n",
    "num_dim = 3\n",
    "prior = utils.BoxUniform(low=-2 * torch.ones(num_dim), high=2 * torch.ones(num_dim))\n",
    "\n",
    "def simulator(parameter_set):\n",
    "    return 1.0 + parameter_set + torch.randn(parameter_set.shape) * 0.1\n",
    "\n",
    "posterior = infer(simulator, prior, method=\"SNPE_A\", num_simulations=10)\n",
    "assert posterior is not None, \"Most basic use of 'infer' failed\"\n",
    "posterior = infer(\n",
    "    simulator,\n",
    "    prior,\n",
    "    method=\"SNPE_A\",\n",
    "    num_simulations=10,\n",
    "    init_kwargs={\"num_components\": 5},\n",
    "    train_kwargs={\"max_num_epochs\": 2},\n",
    "    build_posterior_kwargs={\"prior\": prior},\n",
    ")\n",
    "assert posterior is not None, \"Using 'infer' with keyword arguments failed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from pyro.infer.mcmc import MCMC\n",
    "from torch import Tensor, eye, zeros\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "from sbi.inference import (\n",
    "    SNL,\n",
    "    MCMCPosterior,\n",
    "    likelihood_estimator_based_potential,\n",
    "    simulate_for_sbi,\n",
    ")\n",
    "from sbi.samplers.mcmc import PyMCSampler, SliceSamplerSerial, SliceSamplerVectorized\n",
    "from sbi.simulators.linear_gaussian import diagonal_linear_gaussian\n",
    "from sbi.utils.user_input_checks import process_prior, process_simulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sampling_method: str = \"slice_np_vectorized\"\n",
    "num_chains: int = 4,\n",
    "mcmc_params_fast: dict = {}\n",
    "num_dim: int = 2\n",
    "num_samples: int = 42\n",
    "num_trials: int = 2\n",
    "num_simulations: int = 10\n",
    "\n",
    "x_o = zeros((num_trials, num_dim))\n",
    "mcmc_params_fast[\"num_chains\"] = num_chains\n",
    "\n",
    "prior = MultivariateNormal(loc=zeros(num_dim), covariance_matrix=eye(num_dim))\n",
    "simulator = diagonal_linear_gaussian\n",
    "\n",
    "inference = SNL(prior, show_progress_bars=False)\n",
    "\n",
    "prior, _, prior_returns_numpy = process_prior(prior)\n",
    "simulator = process_simulator(simulator, prior, prior_returns_numpy)\n",
    "theta, x = simulate_for_sbi(\n",
    "    simulator, prior, num_simulations, simulation_batch_size=10\n",
    ")\n",
    "estimator = inference.append_simulations(theta, x).train(max_num_epochs=5)\n",
    "potential_fn, transform = likelihood_estimator_based_potential(\n",
    "    estimator, prior, x_o\n",
    ")\n",
    "posterior = MCMCPosterior(\n",
    "    potential_fn, theta_transform=transform, method=sampling_method, proposal=prior\n",
    ")\n",
    "\n",
    "assert posterior.posterior_sampler is None\n",
    "samples = posterior.sample(\n",
    "    sample_shape=(num_samples, num_chains),\n",
    "    x=x_o,\n",
    "    mcmc_parameters={\"init_strategy\": \"prior\", **mcmc_params_fast},\n",
    ")\n",
    "# assert isinstance(samples, Tensor)\n",
    "# assert samples.shape == (num_samples, num_chains, num_dim)\n",
    "\n",
    "# if \"pyro\" in sampling_method:\n",
    "#     assert type(posterior.posterior_sampler) is MCMC\n",
    "# elif \"pymc\" in sampling_method:\n",
    "#     assert type(posterior.posterior_sampler) is PyMCSampler\n",
    "# elif sampling_method == \"slice_np\":\n",
    "#     assert type(posterior.posterior_sampler) is SliceSamplerSerial\n",
    "# else:  # sampling_method == \"slice_np_vectorized\"\n",
    "#     assert type(posterior.posterior_sampler) is SliceSamplerVectorized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior.default_x.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbi3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
